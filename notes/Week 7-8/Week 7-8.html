<html>
<head>
  <title>Evernote Export</title>
  <basefont face="Segoe UI" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="Evernote Windows/307027 (en-US, DDL); Windows/10.0.0 (Win64);"/>
  <style>
    body, td {
      font-family: Segoe UI;
      font-size: 10pt;
    }
  </style>
</head>
<body>
<a name="565"/>
<h1>Message Passing Interface (MPI)</h1>
<div>
<table bgcolor="#D4DDE5" border="0">
<tr><td><b>Created:</b></td><td><i>8/05/2018 1:17 PM</i></td></tr>
<tr><td><b>Updated:</b></td><td><i>14/05/2018 5:50 PM</i></td></tr>
</table>
</div>
<br/>

<div>
<span><div><div><span style="font-weight: bold;">Message Passing</span></div><div><br/></div><div>Shared memory space is fine, until you have processes or threads across the network. This requires some sort of communication protocol.</div><div><br/></div><div>Such system is called <span style="font-weight: bold;">Distributed Memory</span>. Since each thread or process has its own memory, there is no worry regarding race conditions.</div><div><br/></div><div><img src="Week 7-8_files/Image.png" type="image/png" data-filename="Image.png"/></div><div><br/></div><div><span style="font-weight: bold;">Two methods for communication</span></div><div><br/></div><ul><li><div><span style="font-style: italic;">Synchronous</span>: threads sending &amp; receiving messages are blocked until the message is sent.</div></li><li><div><span style="font-style: italic;">Asynchronous:</span> threads will not block.</div></li><li><div><span style="font-style: italic;">Rendezvous</span>: implements Request-Reply. Asynchronous receiving and synchronous reply. So a thread waits until the other thread replies.</div></li></ul><div><br/></div><div><br/></div><div><span style="font-weight: bold;">MPI in Java using MPJ</span></div><div><br/></div><div>MPJ offers the following methods:</div><ol><li><div><span style="font-style: italic;">Send</span>: blocks while sending the message to the process.</div></li><li><div><span style="font-style: italic;">Recv</span>: blocks until it receives.</div></li><li><div><span style="font-style: italic;">Isend</span>: non-blocking send.</div></li><li><div><span style="font-style: italic;">Irecv</span>: non-blocking receiving.</div></li><li><div><span style="font-style: italic;">Bcast</span>: sends a global message.</div></li><li><div>MPI.Wtime() returns the current time stamp.</div></li></ol><div><br/></div><div><br/></div><div><span style="font-weight: bold;">Collective Communication</span></div><div><img src="Week 7-8_files/Image [1].png" type="image/png" data-filename="Image.png"/></div><div>Processes can combine messages to create a structure. <b>The load is furthermore balanced</b> across processes speeding up the process since the system is using parallelism.</div><div><br/></div><div>There are functions such as <span style="font-style: italic;">MPI.SUM</span> used to combine data from many processes. <span style="font-style: italic;">MPI.Allreduce</span> transmits messages to all processes.  <i>MPI.reduce</i> passes the content onto a single process.</div><div><br/></div><div><span style="font-weight: bold;">Safety</span></div><div>Deadlock is possible. If unsure if process can deadlock, use <span style="font-style: italic;">synchronized</span>. <span style="font-style: italic;">MPI.Ssend</span> enforces <span style="font-style: italic;">synchronized</span>.</div></div><div><br/></div></span>
</div>
<hr>
<a name="597"/>
<h1>Message Passing Java (MPJ) ...more</h1>
<div>
<table bgcolor="#D4DDE5" border="0">
<tr><td><b>Created:</b></td><td><i>22/05/2018 2:59 PM</i></td></tr>
<tr><td><b>Updated:</b></td><td><i>22/05/2018 6:02 PM</i></td></tr>
</table>
</div>
<br/>

<div>
<span><div><div><span style="font-weight: bold;">Some more on Message Passing</span></div><div>The rank is the current process' ID. Each process has its own ID which uniquely identifies it. The size is the total number of processes specified when running with <b><i>-np {NUMBER OF PROCESSES}</i></b> as a parameter.</div><div><img src="Week 7-8_files/Image [2].png" type="image/png" data-filename="Image.png" width="556"/></div><div><span style="font-weight: bold;"><br/></span></div><div>The <a href="http://mpi.comm_world/">MPI.COMM_WORLD</a> class is the communicator which handles the parallelism behind the communication between the processes. This class contains the methods <i>Send, Recv, Isend, Irecv, Bcast, Reduce, Gather, Scatter, and Barrier. I at beginning is non-blocking.</i></div><div><i><br/></i></div><div><b>Example usage in terms of scientific programming</b></div><div>The trapezoidal rule for example approximates the area underneath a curve. The number, n, is how many times the area underneath the curve is divided. This is also the number of processes for example, that will calculate their own individual areas.</div><div><img src="Week 7-8_files/Image [3].png" type="image/png" data-filename="Image.png" width="411"/></div><ol><li><div>partition the solution into tasks.</div></li><ol><li><div>find the area of an individual trapezoid.</div></li><li><div>sum the areas.</div></li></ol><li><div>identify the communication channels between the tasks: pass the final areas to the process which sums the lot.</div></li><li><div>aggregate the tasks into composite tasks.</div></li><ol><li><div>how many processes are needed? n</div></li><li><div>split the range (b - a) by n.</div></li></ol></ol><div><br/></div><div>Each process will run the same implemented function. Therefore, the rank can be used to determine the value of x for performing the calculation. While rank == 0, is the root process summing, the other process send to the root process. Note MPI.DOUBLE is simply the data-type declaration.</div><div><br/></div><div><img src="Week 7-8_files/Image [4].png" type="image/png" data-filename="Image.png" width="423"/></div><div><b><br/></b></div><div><b>WARNING (Deadlock)</b></div><div>Deadlock can occur with the BLOCKING version of COMM_WORLD.Send, so use ISend for non-blocking. There are also <b>synchronized</b> versions: MPI.COMM_WORLD.Ssend</div><div><br/></div><div><br/></div><div><b>Efficiency (Scatter, Gather, Collective Communication, Reduce, Allreduce)</b></div><div>In these example, process rank = 0 deals with all the messages. However, this isn't exactly efficient. This is when <i>Collective Communication</i> plays a role. It's a tree-based structure, where messages are passed along as they are finished. The load is somewhat more balanced between processes. This implies that there is more work done in parallel, which makes the program more efficient.</div><div><br/></div><div><img src="Week 7-8_files/Image [5].png" type="image/png" data-filename="Image.png" width="212"/></div><div>There is a method in MPI.COMM_WORLD, Reduce (the following is for C).</div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div>//Function prototype</div><div>MPI_Reduce(</div><div>    void* send_data, <span>    </span>//An array of data to send</div><div>    void* recv_data, <span>    </span>//Needed for Root process, to recv data. size: sizeof(datatype) * count</div><div>    int count, <span>    </span><span>    <span>  </span></span>//Total # of processes</div><div>    MPI_Datatype datatype,<span>    //The data type of the data (i.e MPI.DOUBLE)</span></div><div>    MPI_Op op,<span>    </span><span>    </span><span>    //The operation to perform, i.e MPI.SUM</span></div><div>    int root,<span>    </span><span>    </span><span>    //The Rank of the root process</span></div><div>    MPI_Comm communicator</div><div>)</div></div><div><font style="color: rgb(0, 0, 255);"><u>Refer to ./MPI/mpj_reduce.pdf</u></font></div><div><font>MPI.COMM_WORLD.Allreduce transmits to all processes, rather than just root.</font></div><div><font><a href="http://mpi.comm_world.scatter/">MPI.COMM_WORLD.Scatter</a> and <a href="http://mpi.comm_world.gather/">MPI.COMM_WORLD.Gather</a> scatter and gather data arrays from multiple processes. Scatter to give, gather to collect the results.</font></div><div><span style="font-weight: bold;"><br/></span></div><div><span style="font-weight: bold;"><br/></span></div><div><span style="font-weight: bold;"><br/></span></div><div><span style="font-weight: bold;">Run configuration (IntelliJ IDEA)</span></div><div><img src="Week 7-8_files/Image [6].png" type="image/png" data-filename="Image.png" width="559"/></div><div><br/></div></div></span>
</div></body></html> 